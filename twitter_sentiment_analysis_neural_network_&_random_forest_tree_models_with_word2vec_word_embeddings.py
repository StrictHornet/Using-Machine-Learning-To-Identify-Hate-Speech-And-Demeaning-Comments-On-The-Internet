# -*- coding: utf-8 -*-
"""Twitter Sentiment Analysis Neural Network & Random Forest Tree Models with Word2Vec Word Embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qjBn1wIS-7J0Tg2hiG-R_wKhTrGj6Ky9

Importing Libraries
"""

import pandas as pd
import numpy as np  
import nltk
import spacy
from spacy.tokenizer import Tokenizer
import re
import matplotlib.pyplot as plt 
from wordcloud import WordCloud
from nltk.stem import WordNetLemmatizer
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english") # english defined stemmer
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

"""Importing Dataset"""

train_dataset = pd.read_csv("train_dat.csv") #Train dataset
train_dataset = train_dataset.drop(['id'], axis=1)
test_dataset = pd.read_csv("test_dat.csv") #Test dataset
test_dataset = test_dataset.drop(['id'], axis=1)

"""User-defined Functions"""

def Pattern(text, pattern):
  matched = re.findall(pattern, text) # find text that matches pattern parameter
  for match in matched:
    text = re.sub(match, "", text)
  return text


stop_words = set(stopwords.words('english'))
def Stopwords(text):
    return " ".join([word for word in str(text).split() if word not in stop_words]) ############################################## return " ".join([word for word in str(text).split() if word not in stop_words])

# Define custom spaCY tokenizer
sp = spacy.load("en_core_web_sm")
prefix_re = re.compile('''^\$#[a-zA-Z0-9]''')
def custom_tokenizer(nlp):
    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search)
sp.tokenizer = custom_tokenizer(sp)

def Lemma(text):
  text = " ".join(text)
  text = sp(text)
  all = []
  for word in text:
    try:
      nword = str(word.lemma_)
      #nword = word
    except:
      nword = word
    all.append(nword)
    #x = np.asarray(all)
  return all

def Preprocess(dataset):  
  dataset["tweet"] = np.vectorize(Stopwords)(dataset["tweet"]) # remove stopwords from tweets
  dataset["tweet"] = np.vectorize(Pattern)(dataset["tweet"], "@[\w]*") # vecotorize "Pattern" function to allow it remove @username across multiple arrays
  dataset["tweet"] = dataset["tweet"].str.replace("[^a-zA-Z!?]", " ") # remove and replace all characters that are not in the alphabet range, #, ! or ?
  dataset["tweet"] = dataset["tweet"].apply(lambda twt: " ".join([word for word in twt.split() if len(word) > 2 ])) # split the tweet then replace words shorter than 2 aplhabets
  dataset["tweet"] = dataset["tweet"].apply(lambda twt: re.sub(" +", " ", twt)) # remove additional spaces
  dataset["tweet"] = dataset["tweet"].apply(lambda twt: twt.split()) # split tweet into individual words
  dataset["tweet"] = dataset["tweet"].apply(Lemma)
  #d = np.vectorize(Lemma)(dataset["tweet"]) # Lemmatization carried out on individual words of the tweets
#dataset["tweet"] = d

#print(dataset.head(10))

"""Preprocessing"""

#train_dataset = train_dataset.iloc[0:2000, :]
Preprocess(train_dataset)
print(train_dataset.head(20))

#Preprocess(train)

print(train_dataset)

"""Further Preprocessing For Word Embeddings"""

#dataset = train_dataset.drop(['id'], axis=1)
#dataset = dataset.dropna()
#print(dataset)
X = train_dataset.iloc[:, 1].values
y = train_dataset.iloc[:, 0].values

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 99)

#print(dataset)
print(X)

print(x_train)

"""Word Embeddings"""

from gensim.models import Word2Vec 
n_dim = 300

##------------------------------------------ Loading Pretrained Vectors
import gensim.downloader as api
word2vec = api.load('word2vec-google-news-300')
##------------------------------------------

#Build word vector for training set by using the average value of all word vectors in the tweet, then scale
#-----------------------------------------------------------------------------------------------------------
# def Vectorizer(text, size):
#     vec = np.zeros(size).reshape((1, size))
#     count = 0.
#     for any_word in text:
#         try:
#             vec += word2vec.wv[any_word].reshape((1, size))
#             count += 1.
#         except KeyError:
#             continue
#     if count != 0:
#         vec /= count
#     # print(vec)
#     # print(text)
#     # input("Press Enter to continue...")    
#     return vec

def Vectorizer(tweet):
  word_count = 0.
  word_vector = np.zeros(n_dim).reshape((1, n_dim))

  for word in tweet:
    try:
      word_vector += word2vec.wv[word].reshape((1,n_dim))
      word_count += 1.
    except KeyError:
      continue
  if word_count != 0:
    tweet_vector = word_vector / word_count

  return tweet_vector

#Build train vectors then standardize vectors
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
train_vecs = np.concatenate([Vectorizer(z, n_dim) for z in x_train])
train_vecs = scaler.fit_transform(train_vecs)

#Build test tweet vectors then standardize vectors
test_vecs = np.concatenate([Vectorizer(z, n_dim) for z in x_test])
test_vecs = scaler.transform(test_vecs)

print(test_vecs.shape)

"""Neural Network"""

from sklearn.neural_network import MLPClassifier
clf = MLPClassifier(solver='sgd', random_state=42)#hidden_layer_sizes=(10, 10),
clf.fit(train_vecs, y_train)
y_pred = clf.predict(test_vecs)
# print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)
# from sklearn.model_selection import cross_val_score
# acc = cross_val_score(estimator = clf, X = train_vecs, y = y_train, cv = 10)
# print("Accuracy {:.2f}%".format(acc.mean()*100))
# print("Standard Deviation {:.2f} %".format(acc.std()*100))

from sklearn.metrics import f1_score
print(f1_score(y_test, y_pred, pos_label=1))
print(f1_score(y_test, y_pred, pos_label=0))

from sklearn.feature_extraction.text import CountVectorizer # feature extraction for bag of words model
from sklearn.linear_model import LogisticRegression # ML Algorithm for BOW Model
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, auc
#from sklearn.metrics import precision_recall_curve

y_pred = clf.predict_proba(test_vecs)[:,1]

fpr,tpr,_ = roc_curve(y_test, y_pred)
roc_auc = auc(fpr,tpr)
plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.legend(loc='upper left')

plt.show()

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)
clf.fit(train_vecs, y_train)
y_pred = clf.predict(test_vecs)
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)
# from sklearn.model_selection import cross_val_score
# acc = cross_val_score(estimator = classifier, X = train_vecs, y = y_train, cv = 10)
# print("Accuracy {:.2f}%".format(acc.mean()*100))
# print("Standard Deviation {:.2f} %".format(acc.std()*100))

from sklearn.metrics import f1_score
print(f1_score(y_test, y_pred, pos_label=1))
print(f1_score(y_test, y_pred, pos_label=0))

i = 0
count = 0
for item in y_pred:
  if item == 1:
    count+=1
  #print(item, y_test[i])
  i+=1
print(count)
print(cm)

y_pred = clf.predict_proba(test_vecs)[:,1]

fpr,tpr,_ = roc_curve(y_test, y_pred)
roc_auc = auc(fpr,tpr)
plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.legend(loc='upper left')

plt.show()

"""# New Section

-------------------------------------------------------------------------------

BOW Logistic Regression
"""

clf = LogisticRegression() # assigning classifier to "clf"
clf.fit(train_vecs, y_train) # fitting the classifier to the dataset

y_pred = clf.predict_proba(test_vecs) # predicting probability of test sample
y_pred = y_pred[:,1] >= 0.5 # if probability is above .199999 classify as toxic tweet (low treshold due to imbalance in dataset)

print(f1_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

"""BOW Logistic Regression ROC"""

y_pred = clf.predict_proba(test_vecs)[:,1]

fpr,tpr,_ = roc_curve(y_test, y_pred)
roc_auc = auc(fpr,tpr)
plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.legend(loc='upper left')

plt.show()

from sklearn.svm import SVC
classifier = SVC(kernel = "linear", random_state = 99, probability=True)
classifier.fit(train_vecs, y_train)
y_pred = classifier.predict(test_vecs)
# print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)
# from sklearn.model_selection import cross_val_score
# acc = cross_val_score(estimator = classifier, X = train_vecs, y = y_train, cv = 10, scoring="f1")
# print("Accuracy {:.2f}%".format(acc.mean()*100))
# print("Standard Deviation {:.2f} %".format(acc.std()*100))
print(f1_score(y_test, y_pred, pos_label=1))
print(f1_score(y_test, y_pred, pos_label=0))
print(type(y_test))
print(y_pred)

y_pred = classifier.predict_proba(test_vecs)[:,1]

fpr,tpr,_ = roc_curve(y_test, y_pred)
roc_auc = auc(fpr,tpr)
plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.legend(loc='upper left')

plt.show()

def keeprunning():
  i = 1
  while i > 0:
    i+=1

run = 1
while run == 1:
  keeprunning()

print(i)